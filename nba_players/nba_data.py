# -*- coding: utf-8 -*-
"""NBA Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gj-tRWLecSzHtWE7Z-5S72X-AN3_nOKo

## Downloading NBA Player Data from Kaggle

To forecast NBA player performance, we begin by sourcing historical player statistics. This dataset, available on Kaggle, contains individual-level season stats for thousands of players across multiple decades.

We use the kagglehub library to access and download this dataset programmatically. This allows for reproducibility and seamless integration into our analysis pipeline.
"""

import kagglehub

path = kagglehub.dataset_download("justinas/nba-players-data")

print("Path to dataset files:", path)

import pandas as pd
nba_data = pd.read_csv("/root/.cache/kagglehub/datasets/justinas/nba-players-data/versions/5/all_seasons.csv")
nba_data.head(11)

"""## Basic Summary Statistics

To get a foundational understanding of the NBA player dataset, we calculate basic descriptive statistics for key numeric columns:

- **Age**: Represents player experience and development stage.
- **Player Height**: Often correlates with position and role on the court.
- **Points Per Game (PTS)**: A core measure of individual performance.

For each of these, we compute the **mean**, **median**, and **mode** to understand the central tendencies and potential distribution skew. These statistics serve as a baseline for any further preprocessing or modeling steps.

"""

mean_age = nba_data['age'].mean()
mean_height = nba_data['player_height'].mean()
mean_pts = nba_data['pts'].mean()
mode_age = nba_data['age'].mode()
mode_height = nba_data['player_height'].mode()
mode_pts = nba_data['pts'].mode()
median_age = nba_data['age'].median()
median_height = nba_data['player_height'].median()
median_pts = nba_data['pts'].median()
mode_age_list = mode_age.tolist()
mode_height_list = mode_height.tolist()
mode_pts_list = mode_pts.tolist()


print(f"Age has a mean of {mean_age}, a median of {median_age}, and a mode of {mode_age_list}")
print(f"Height has a mean of {mean_height}, a median of {median_height}, and a mode of {mode_height_list}")
print(f"Pts has a mean of {mean_pts}, a median of {median_pts}, and a mode of {mode_pts_list}")

"""### Data Integrity and Structure Check

Before diving deeper into modeling or analysis, it's important to examine the dataset's structure and quality. This section includes three essential checks:

### Data Types Overview
We begin by inspecting the **data types** of each column using .dtypes. This helps verify if each feature is properly classified (e.g., numerical, categorical, object). Detecting incorrect types early (e.g., numerical data stored as strings) prevents downstream errors.
"""

nba_data.dtypes

"""### Missing Values

We use .isnull().sum() to **identify missing data** across the dataset. Columns with a significant number of null values may require imputation, removal, or domain-specific judgment to ensure model accuracy and integrity.
"""

nba_data.isnull().sum()

"""
### Data Sorting

To highlight standout performers, we **sort the dataset by points per game (pts) in descending order**. This simple sort gives a quick look at the league’s top scorers and helps prioritize analysis around high-impact players.
"""

nba_data.sort_values(by = 'pts', ascending=False)

"""## Categorical Handling, Filtering, and Aggregation

This section focuses on preparing categorical data, filtering subsets of interest, and performing summary statistics across groups.

### Categorical Data Encoding
To prepare categorical features like 'team_abbreviation' and 'college' for machine learning models, we apply **one-hot encoding**. This converts categorical values into binary columns, making them interpretable by algorithms that require numerical input.
"""

categorical_columns = ['team_abbreviation', 'college']
nba_data_encoded = pd.get_dummies(nba_data, columns=categorical_columns)
nba_data_encoded.head()

"""### Data Filtering
We isolate players from select colleges. For example, **North Carolina** and **Duke** to perform targeted analysis or comparisons. This highlights how subsets can be carved out for deeper domain-specific insights.
"""

filtered_data = nba_data[(nba_data['college'] == 'North Carolina') | (nba_data['college'] == 'Duke')]
filtered_data

"""## Aggregation: Team-Based Player Metrics

In this step, we calculate aggregated statistics for each NBA team to better understand the physical and demographic makeup of their rosters.

We compute the following metrics for every team_abbreviation:
- **Player Height**: Mean, Maximum, and Minimum
- **Player Weight**: Mean, Maximum, and Minimum
- **Player Age**: Mean, Maximum, and Minimum

This helps us identify trends like:
+ Which teams tend to have the tallest players?
+ Which teams skew older or younger?
+ How physical characteristics differ across franchises

These aggregated insights can serve as a baseline for further performance or health analytics.

"""

aggregated_heights = nba_data.groupby('team_abbreviation')['player_height'].agg(['mean', 'max', 'min'])
aggregated_weight = nba_data.groupby('team_abbreviation')['player_weight'].agg(['mean', 'max', 'min'])
aggregated_age = nba_data.groupby('team_abbreviation')['age'].agg(['mean', 'max', 'min'])


aggregated_heights, aggregated_weight, aggregated_age

"""## Visualization: Distributions of Key Variables

To gain a quick understanding of the dataset's characteristics, we visualized the distributions of three important numerical features: **Age**, **Points per Game (PTS)**, and **Player Height**.

### Histogram Plots
- **Age**: Skewed toward younger players, with a peak in the mid-20s.
- **Points**: Right-skewed distribution, suggesting most players score under 10 PPG, with a few high performers pulling the average up.
- **Height**: Roughly normal distribution centered around 205 cm (~6'9").

### Box Plots
- Help identify outliers in each feature.
- Reinforce that while most players fall within expected ranges, there are outliers — particularly in scoring.

These visualizations help establish baseline expectations and uncover trends or anomalies worth deeper investigation.

"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 5))


# AGE DISTRIBUTION
plt.subplot(1, 3, 1)
sns.histplot(nba_data['age'], kde=True, color='blue')
plt.title('Age Distribution')

# PTS DISTRIBUTION
plt.subplot(1, 3, 2)
sns.histplot(nba_data['pts'], kde=True, color='grey')
plt.title('Points Distribution')

# HEIGHT DISTRIBUTION
plt.subplot(1, 3, 3)
sns.histplot(nba_data['player_height'], kde=True, color='red')
plt.title('Player Height Distribution')

plt.figure(figsize=(15, 5))
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 5))

# AGE
plt.subplot(1, 3, 1)
sns.boxplot(y=nba_data['age'], color='blue')
plt.title('Age Box Plot')

# PTS
plt.subplot(1, 3, 2)
sns.boxplot(y=nba_data['pts'], color='grey')
plt.title('Points Box Plot')

# HEIGHT
plt.subplot(1, 3, 3)
sns.boxplot(y=nba_data['player_height'], color='red')
plt.title('Player Height Box Plot')

plt.tight_layout()
plt.show()

"""### Correlation Analysis

To understand how core performance metrics relate to one another, we conducted a correlation analysis on:

- **Points per Game (PTS)**
- **Rebounds (REB)**
- **Assists (AST)**
- **Net Rating**

The heatmap below highlights the strength and direction of these relationships:

- **PTS vs AST**: Moderately positive, suggesting higher scorers often contribute with assists.
- **PTS vs REB**: Weak correlation, indicating scoring and rebounding may operate more independently.
- **Net Rating** shows a mild to moderate relationship with other variables, reflecting its aggregate nature as a team performance metric.
"""

relevant_columns = nba_data[['pts', 'reb', 'ast', 'net_rating']]

correlation_matrix = relevant_columns.corr()

correlation_matrix

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation between PTS, REB, AST, and Net Rating')
plt.show()

"""### Grouping and Aggregation

To explore team and draft-year trends, we calculate the average **points (PTS)**, **rebounds (REB)**, and **assists (AST)** for each group of players, segmented by their:

- draft_year
- team_abbreviation

This allows us to:
- Analyze how draft classes have performed over time.
- Compare how different teams have developed or utilized their drafted talent.
- Identify eras or teams with especially strong or weak player contributions.

The output provides a structured overview of per-team player productivity across multiple seasons.

"""

grouped_data = nba_data.groupby(['draft_year', 'team_abbreviation']).agg({
    'pts': 'mean',
    'reb': 'mean',
    'ast': 'mean'
}).reset_index()

grouped_data.rename(columns={
    'pts': 'avg_pts',
    'reb': 'avg_reb',
    'ast': 'avg_ast'
}, inplace=True)

grouped_data

"""## Time Series Analysis

This section tracks the **performance trajectory** of a single player — in this case, **Ja Morant** — by analyzing how his **points per game (PTS)** have evolved across seasons.

Steps included:
- Filtering the dataset to isolate Ja Morant’s statistics.
- Sorting by season to maintain chronological order.
- Plotting his pts (points per game) across each season.

This type of time series visualization helps:
- Evaluate player development year-over-year.
- Identify performance trends, peaks, or regressions.
- Inform decisions for fantasy leagues, trades, or historical comparisons.

"""

player_name = "Ja Morant"
player_data = nba_data[nba_data['player_name'] == player_name]
player_data = player_data.sort_values('season')
player_data.set_index('season', inplace=True)

plt.figure(figsize=(10, 5))
plt.plot(player_data['pts'], marker='o', linestyle='-')
plt.title(f"Points per Game Over Seasons for {player_name}")
plt.xlabel('Season')
plt.ylabel('Points per Game (PTS)')
plt.xticks(rotation=45)
plt.grid(visible=True, which='both', color='gray', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

"""## Text Processing: College Insights

This section explores the college column to extract simple but useful textual insights.

- **Most Common College:** We determine which college has produced the highest number of NBA players in the dataset.
- **Unique Colleges Count:** We compute how many different colleges are represented overall.

This analysis helps highlight schools with strong pipelines to the NBA and provides context for scouting or historical comparisons.

"""

most_common_college = nba_data['college'].mode()[0]
print(f"The most common college is: {most_common_college}")

unique_colleges = nba_data['college'].nunique()
print(f"The number of unique colleges is: {unique_colleges}")

"""## Predictive Modeling: Linear Regression

In this section, we develop a simple **linear regression model** to predict a player's points per game (pts) based on physical and performance metrics.

#### Features Used:
- age
- player_height
- player_weight
- gp (games played)
- reb (rebounds)
- ast (assists)

#### Modeling Workflow:
1. Split the dataset into training and testing sets (80/20 split).
2. Train a linear regression model on the training set.
3. Evaluate model performance using:
   - **Mean Squared Error (MSE)**: Measures average squared difference between actual and predicted values.
   - **R² Score**: Indicates the proportion of variance explained by the model.

This model provides a foundational approach for understanding how basic player metrics correlate with scoring output in the NBA.

"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

features = ['age', 'player_height', 'player_weight', 'gp', 'reb', 'ast']

X = nba_data[features]
y = nba_data['pts']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

model = LinearRegression()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print('Coefficients: \n', model.coef_)

print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))

print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))

"""##bCluster Analysis: K-Means Clustering of NBA Players

In this section, we apply **K-Means Clustering** to group NBA players based on a variety of performance metrics. This unsupervised learning approach helps us discover natural groupings or "player archetypes" in the data.

#### Features Used:
- pts (points)
- reb (rebounds)
- ast (assists)
- net_rating (overall contribution to team performance)
- oreb_pct, `dreb_pct (offensive/defensive rebound percentage)
- usg_pct (usage rate)
- ts_pct (true shooting percentage)
- ast_pc` (assist rate)

#### Workflow:
1. **Preprocessing:**
   - Handle missing values with mean imputation.
   - Standardize the features to have zero mean and unit variance.

2. **Modeling:**
   - Fit a K-Means model with k=5 clusters.
   - Assign cluster labels back to the original dataset.
   - Calculate mean feature values per cluster to interpret each group’s characteristics.

3. **Visualization:**
   - Use PCA to reduce dimensions for visualization.
   - Plot the clusters in 2D space to examine separation and density.

This method allows us to segment players based on statistical profiles — useful for scouting, team construction, and strategic matchups.

"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

metrics = ['pts', 'reb', 'ast', 'net_rating', 'oreb_pct', 'dreb_pct', 'usg_pct', 'ts_pct', 'ast_pct']
data_for_clustering = nba_data[metrics]
data_for_clustering.fillna(data_for_clustering.mean(), inplace=True)
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)


kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(data_for_clustering_scaled)
labels = kmeans.labels_
nba_data['cluster'] = labels

print(nba_data['cluster'].value_counts())
cluster_means = nba_data.groupby('cluster')[metrics].mean()
print(cluster_means)


pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data_for_clustering_scaled)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=nba_data['cluster'], alpha=0.5, cmap='viridis')

plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Clusters of NBA Players')
plt.colorbar(scatter, label='Cluster')
plt.show()

"""## Classification: Predicting High-Impact vs. Low-Impact Colleges

In this section, we build a **Random Forest Classifier** to determine whether a player’s college should be labeled as “High-impact” or “Low-impact” based on the performance of its alumni.

#### Objective:
To assess the long-term player output from each college by calculating the **average points per game (PTS)** across all its players. Colleges in the **top 25%** (above the 75th percentile) are considered "High-impact."

#### Workflow:
1. **Label Generation:**
   - Aggregate average PTS by college.
   - Define a binary target variable: "High-impact" or "Low-impact".

2. **Feature Selection:**
   - Use relevant performance metrics like age, height, weight, PTS, REB, AST, net_rating, usg_pct, etc.

3. **Preprocessing:**
   - Apply **StandardScaler** to normalize input features.

4. **Modeling:**
   - Use **Random Forest Classifier** for its robustness and ability to handle non-linear relationships.

5. **Evaluation:**
   - Output includes a **confusion matrix** and **classification report**, showcasing the model’s accuracy, precision, recall, and F1-score.

This model helps quantify the historical performance impact of college programs on NBA success and can guide scouting or talent evaluation decisions.

"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

nba_data = nba_data.dropna(subset=['college'])

college_performance = nba_data.groupby('college')['pts'].mean()
high_impact_threshold = college_performance.quantile(0.75)
nba_data['college_impact'] = nba_data['college'].apply(lambda x: 'High-impact' if college_performance[x] >= high_impact_threshold else 'Low-impact')
features = ['age', 'player_height', 'player_weight', 'pts', 'reb', 'ast', 'net_rating', 'oreb_pct', 'dreb_pct', 'usg_pct', 'ts_pct', 'ast_pct']
X = nba_data[features]
y = nba_data['college_impact']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## Time Series Forecasting: Predicting Player Points Using ARIMA and LSTM

In this section, we forecast **Ja Morant’s** points for the upcoming NBA season using two time series modeling techniques: **ARIMA** and **LSTM**.

### Data Preparation
- Converted the season string (e.g., "2018-19") into a datetime format based on the start year.
- Filtered and sorted Ja Morant’s records by season_start_date for accurate time series sequencing.
- Used pts (points per game) as the target metric.
"""

nba_data['season_start_year'] = nba_data['season'].apply(lambda x: int(x.split('-')[0]))
nba_data['season_start_date'] = pd.to_datetime(nba_data['season_start_year'], format='%Y')

nba_data['pts'] = pd.to_numeric(nba_data['pts'], errors='coerce')
nba_data = nba_data.sort_values('season_start_date')

player_name = 'Ja Morant'
player_data = nba_data[nba_data['player_name'] == player_name]
player_data.set_index('season_start_date', inplace=True)

"""### ARIMA Model
- **ARIMA (1,1,1)** was selected for its balance between autoregression, differencing, and moving average.
- This statistical model is suited for short-term forecasting with limited historical data.
- Forecasted next season’s point average with a simple .forecast(steps=1).
"""

from statsmodels.tsa.arima.model import ARIMA

arima_model = ARIMA(player_data['pts'], order=(1,1,1))
arima_results = arima_model.fit()

arima_forecast = arima_results.forecast(steps=1)[0]
print(f"ARIMA forecast for {player_name}'s pts in the next season: {arima_forecast}")

"""

### LSTM Model
- Scaled the pts series using **MinMaxScaler** to improve LSTM training performance.
- Created lag-based sequences with a **2-time step window** to capture short-term dependencies.
- Built a 2-layer **LSTM** model followed by a **dense output layer** for regression.
- Trained over **100 epochs** with batch size 1 to simulate time series learning.
- Inverse-transformed the prediction back to the original scale to interpret results.
"""

# LTSM MODEL

from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_pts = scaler.fit_transform(player_data['pts'].values.reshape(-1,1))

def create_dataset(dataset, time_step=1):
    X, Y = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]
        X.append(a)
        Y.append(dataset[i + time_step, 0])
    return np.array(X), np.array(Y)

time_step = 2
X, y = create_dataset(scaled_pts, time_step)
X = X.reshape(X.shape[0], X.shape[1], 1)

lstm_model = Sequential()
lstm_model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))
lstm_model.add(LSTM(50))
lstm_model.add(Dense(1))

lstm_model.compile(loss='mean_squared_error', optimizer='adam')
lstm_model.fit(X, y, epochs=100, batch_size=1, verbose=1)

lstm_forecast_scaled = lstm_model.predict(X[-1].reshape(1, time_step, 1))
lstm_forecast = scaler.inverse_transform(lstm_forecast_scaled)
print(f"LSTM forecast for {player_name}'s pts in the next season: {lstm_forecast[0][0]}")

"""## Deep Learning: Predicting NBA Player Performance with a Neural Network

In this section, we use a simple feedforward neural network to **predict NBA players' points per game (PTS)** based on physical attributes and advanced performance metrics.

#### Objective
Train a regression model to learn the nonlinear relationships between features like height, usage rate, assists, etc., and the player's scoring output.

### Feature Set
We selected 12 key features such as:
- age, player_height, player_weight
- Game performance: gp, reb, ast
- Advanced stats: net_rating, usg_pct, ts_pct, etc.

These were **standardized using StandardScaler** for better convergence during training.
"""

features = ['age', 'player_height', 'player_weight', 'gp', 'reb', 'ast', 'net_rating', 'oreb_pct', 'dreb_pct', 'usg_pct', 'ts_pct', 'ast_pct']
X = nba_data[features]
y_regression = nba_data['pts']

X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### Model Architecture
- **Input Layer:** Matches the number of selected features (12).
- **Hidden Layers:**
  - 64 neurons (ReLU)
  - 32 neurons (ReLU)
- **Output Layer:** 1 neuron (Linear activation) for continuous regression output.
- **Loss Function:** Mean Squared Error (MSE)
- **Optimizer:** Adam (adaptive learning rate)
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))


model.compile(optimizer='adam', loss='mean_squared_error')

"""### Evaluation
- The model was trained for **100 epochs** with a batch size of 10.
- Scatter plot of **actual vs. predicted PTS** shows prediction accuracy.
- A histogram of residuals helps visualize prediction errors and bias.

"""

model.fit(X_train_scaled, y_train_reg, epochs=100, batch_size=10, verbose=1)

loss_reg = model.evaluate(X_test_scaled, y_test_reg, verbose=1)
print('Regression Loss:', loss_reg)

y_pred_reg = model.predict(X_test_scaled)

plt.figure(figsize=(10, 6))
plt.scatter(y_test_reg, y_pred_reg, alpha=0.5)
plt.title('Actual vs. Predicted')
plt.xlabel('Actual Points')
plt.ylabel('Predicted Points')
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=4)
plt.show()

residuals = y_test_reg - y_pred_reg.flatten()

plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=20, alpha=0.7, color='blue')
plt.title('Histogram of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

"""## Sample Player Inference

In this final step, we test the trained neural network by feeding it a sample player's profile. The player has:

- Age: 26  
- Height: 200 cm  
- Weight: 95 kg  
- Games Played: 70  
- Rebounds per game: 6.0  
- Assists per game: 5.0  
- Net Rating: 5.5  
- Offensive Rebound %: 3.0%  
- Defensive Rebound %: 15.0%  
- Usage Rate: 22.0%  
- True Shooting %: 58.0%  
- Assist Rate: 20.0%

These features are scaled and passed into the model, which outputs the predicted number of points per game.

This allows us to evaluate model performance on real-like test cases and demonstrates its capability to support hypothetical player evaluations and scouting insights.

"""

features = ['age', 'player_height', 'player_weight', 'gp', 'reb', 'ast',
            'net_rating', 'oreb_pct', 'dreb_pct', 'usg_pct', 'ts_pct', 'ast_pct']
test_player = np.array([[26, 200, 95, 70, 6.0, 5.0, 5.5, 0.03, 0.15, 0.22, 0.58, 0.20]])
test_player_scaled = scaler.transform(test_player)
predicted_pts = model.predict(test_player_scaled)
print(f"Predicted points per game: {predicted_pts[0][0]:.2f}")