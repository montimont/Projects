# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lk8d6ryQ7kHts6NNpUcpu5cYmno3F14L
"""

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt

data0 = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/Scorecard_Project/Internal_test_0_datasetForMultivariateVSExceptIV.csv', index_col=[0])

data = data0.copy()
target = data['loan_status']
features = data.drop(columns=['loan_status'])

def feature_reduce_xgb(threshold, features, bad):
    '''
    1) Train on an XGBoost classifier and compute the AUC on the same data;
    2) Drop features to keep top features that have threshold (eg: 0.99) feature importance
    3) Repeat 1) 2) until there's only one feature left
    4) Plot the AUC
    '''
    AUC_train = []
    AUC_test = []
    num_features = []

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(features, bad, test_size=0.2, random_state=42)

    while X_train.shape[1] > 1:
        # Define an XGBoost classifier
        fs_clf = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, random_state=42)

        # Train the classifier
        fs_clf.fit(X_train, y_train)

        # Get the feature importance values
        importances = fs_clf.feature_importances_
        feature_names = X_train.columns # list of feature names

        # Sort the feature importances in descending order
        sorted_index = np.argsort(importances)[::-1]

        # Print the sorted feature importance values
        for i in sorted_index:
            print(f"Feature {feature_names[i]}: {importances[i]}")

        # Predict probabilities
        y_train_pred = fs_clf.predict_proba(X_train)[:, 1]
        y_test_pred = fs_clf.predict_proba(X_test)[:, 1]

        # Compute the AUC
        auc_train = roc_auc_score(y_train, y_train_pred)
        auc_test = roc_auc_score(y_test, y_test_pred)

        # Print number of features
        print('Number of Features: {}'.format(X_train.shape[1]))
        # Print the AUC
        print(f"AUC Train: {auc_train:.4f}")
        print(f"AUC Test: {auc_test:.4f}")

        # Record number of features, AUC
        num_features.append(X_train.shape[1])
        AUC_train.append(auc_train)
        AUC_test.append(auc_test)

        # Add up the feature importances until they reach the threshold
        num = 0
        cum_importance = 0.0
        for feature_index in sorted_index:
            cum_importance += importances[feature_index]
            num += 1
            if cum_importance >= threshold:
                break

        # If number of features no longer decrease, decrement 1
        if num_features[-1] == num:
            num -= 1

        # Select the top num features
        top_features = feature_names[sorted_index[:num]]
        X_train = X_train[top_features]
        X_test = X_test[top_features]

    # Plot the AUC
    fig = plt.figure(figsize=(15,8))
    x = np.arange(len(num_features))
    plt.plot(x, AUC_train, label='Train AUC')
    plt.plot(x, AUC_test, label='Test AUC')
    plt.xticks(x, num_features, rotation=90, ha='right', fontsize=6)
    plt.xlabel('Number of Features')
    plt.ylabel('AUC')
    plt.title('AUC vs. Number of Features Curve')
    plt.legend()
    plt.tight_layout()
    plt.show()

feature_reduce_xgb(threshold=0.80, features=features, bad=target)

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [2, 3, 5],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.5, 0.7]
}

# Initialize the XGBClassifier
xgb_clf = XGBClassifier(random_state=42)

grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, scoring='roc_auc',return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best parameters: {best_params}")
print("Hyperparameters\t\t\t\t\t\t\t\t\t\t\t\t\t\tTrain AUC\t\tTest AUC")
for i in range(len(grid_search.cv_results_['params'])):
    params = grid_search.cv_results_['params'][i]
    avg_train_auc = grid_search.cv_results_['mean_train_score'][i]
    avg_test_auc = grid_search.cv_results_['mean_test_score'][i]
    print(f"{params}\t{avg_train_auc:.4f}\t\t{avg_test_auc:.4f}")

print("The best parameters are {}".format(grid_search.best_params_))
print("The mean ROC AUC of the best parameters is {}".format(grid_search.best_score_))

best_params

best_params_updated = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.7}

# Train the model with the best parameters
best_xgb_clf = XGBClassifier(**best_params_updated, random_state=42)
best_xgb_clf.fit(X_train, y_train)

y_train_pred = best_xgb_clf.predict_proba(X_train)[:, 1]
y_test_pred = best_xgb_clf.predict_proba(X_test)[:, 1]

# Compute the AUC
auc_train = roc_auc_score(y_train, y_train_pred)
auc_test = roc_auc_score(y_test, y_test_pred)

print(f"AUC Train: {auc_train:.4f}")
print(f"AUC Test: {auc_test:.4f}")

# Get feature importances
importances = best_xgb_clf.feature_importances_
feature_names = features.columns

feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'XGB_Importance': importances
})

# Sort the DataFrame by importance and add rank
feature_importances = feature_importances.sort_values(by='XGB_Importance', ascending=False).reset_index(drop=True)
feature_importances['XGB_Rank'] = feature_importances['XGB_Importance'].rank(ascending=False).astype(int)

top_features = feature_importances.head(20)
print(top_features)

plt.figure(figsize=(12, 10))
plt.title("Top Feature Importances")
plt.barh(top_features['Feature'], top_features['XGB_Importance'], color='b', align='center')
plt.gca().invert_yaxis()
plt.xlabel("Relative Importance")
plt.show()

feature_importances

feature_importances.to_csv('Internal_XGBoost.csv')